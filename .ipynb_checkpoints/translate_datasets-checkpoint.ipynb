{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnthropicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, item_to_ret='prompt'):\n",
    "        self.hf_dataset = load_dataset('Anthropic/hh-rlhf', split=split)\n",
    "        self.prompts, self.chosen, self.reject = [], [], []\n",
    "        for d in (self.hf_dataset):\n",
    "            try:\n",
    "                prompt, chosen, reject = self.process_response(d)\n",
    "                self.prompts.append(prompt)\n",
    "                self.chosen.append(chosen)\n",
    "                self.reject.append(reject)\n",
    "            except:\n",
    "                continue\n",
    "        self.set_dataset_type(item_to_ret)\n",
    "\n",
    "    def set_dataset_type(self, item_to_ret):\n",
    "        if item_to_ret == 'prompt':\n",
    "            self.dataset = self.prompts\n",
    "        elif item_to_ret == 'chosen':\n",
    "            self.dataset = self.chosen\n",
    "        elif item_to_ret == 'rejected':\n",
    "            self.dataset = self.reject\n",
    "        else:\n",
    "            self.dataset = list(zip(self.prompts, self.chosen, self.reject))\n",
    "\n",
    "    def process_response(self, x):\n",
    "        chosen = x['chosen']\n",
    "        reject = x['rejected']\n",
    "        ind = chosen.rfind('\\n\\nAssistant:')\n",
    "        prompt = chosen[:ind].strip()\n",
    "        assert reject[:len(prompt)] == prompt\n",
    "        chosen = chosen[ind + len('\\n\\nAssistant:'):].strip()\n",
    "        reject = reject[ind + len('\\n\\nAssistant:'):].strip()\n",
    "        return prompt, chosen, reject\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, item_to_ret='prompt'):\n",
    "        self.hf_dataset = load_dataset('openai/summarize_from_feedback', 'comparisons', split=split)\n",
    "        self.prompts, self.chosen, self.reject = [], [], []\n",
    "        for d in (self.hf_dataset):\n",
    "            try:\n",
    "                prompt, chosen, reject = self.process_response(d)\n",
    "                self.prompts.append(prompt)\n",
    "                self.chosen.append(chosen)\n",
    "                self.reject.append(reject)\n",
    "            except:\n",
    "                continue\n",
    "        self.set_dataset_type(item_to_ret)\n",
    "\n",
    "    def set_dataset_type(self, item_to_ret):\n",
    "        if item_to_ret == 'prompt':\n",
    "            self.dataset = self.prompts\n",
    "        elif item_to_ret == 'chosen':\n",
    "            self.dataset = self.chosen\n",
    "        elif item_to_ret == 'rejected':\n",
    "            self.dataset = self.reject\n",
    "        else:\n",
    "            self.dataset = list(zip(self.prompts, self.chosen, self.reject))\n",
    "\n",
    "\n",
    "    def process_response(self, x):\n",
    "        prompt = x['info']['post'].strip()\n",
    "        first = x['summaries'][0]['text'].strip()\n",
    "        second = x['summaries'][1]['text'].strip()\n",
    "        choice = x['choice']\n",
    "        if choice == 0:\n",
    "            return prompt, first, second\n",
    "        elif choice == 1:\n",
    "            return prompt, second, first\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.og_dataset = dataset\n",
    "        self.sentences = []\n",
    "        self.sentence_to_prompt = []\n",
    "        for i,x in enumerate(self.og_dataset):\n",
    "            sents = self.break_sentences(x)\n",
    "            self.sentences.extend(sents)\n",
    "            self.sentence_to_prompt.extend([i] * len(sents))\n",
    "        assert len(self.sentence_to_prompt) == len(self.sentences)\n",
    "\n",
    "    def break_sentences(self, x):\n",
    "        return sent_tokenize(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deberta_test_datasets():\n",
    "    dataset = {}\n",
    "    for t in ['prompt', 'chosen', 'rejected']:\n",
    "        sub_dataset = {}\n",
    "        sub_dataset['openai/summarize_from_feedback'] = SummaryDataset('validation', item_to_ret=t)\n",
    "        # sub_dataset = load_dataset('openai/webgpt_comparisons')\n",
    "        # sub_dataset load_dataset('Dahoas/synthetic-instruct-gptj-pairwise')\n",
    "        #sub_dataset['Anthropic/hh-rlhf'] = load_dataset('Anthropic/hh-rlhf', split='test', item_to_ret=t)\n",
    "        for k,v in sub_dataset.items():\n",
    "            sub_dataset[k] = SentenceDataset(v)\n",
    "        dataset[t] = sub_dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset summarize_from_feedback (/home/ubuntu/.cache/huggingface/datasets/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n",
      "Found cached dataset summarize_from_feedback (/home/ubuntu/.cache/huggingface/datasets/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n",
      "Found cached dataset summarize_from_feedback (/home/ubuntu/.cache/huggingface/datasets/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n"
     ]
    }
   ],
   "source": [
    "datasets = get_deberta_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "prompt\n",
      "openai/summarize_from_feedback 1141862\n",
      "chosen\n",
      "openai/summarize_from_feedback 172862\n",
      "rejected\n",
      "openai/summarize_from_feedback 162461\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets))\n",
    "for k,vs in datasets.items():\n",
    "    print(k)\n",
    "    for k_,v in vs.items():\n",
    "        print(k_, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "#tokenizer_translate = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_translate = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "pipe_translate = pipeline(\"translation\", model=model_name, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1141862 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m all_results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m tqdm(pipe_translate(dataset, batch_size\u001b[39m=\u001b[39mbatch_size), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     all_results\u001b[39m.\u001b[39mappend(out[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtranslation_text\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m results_sub[model_name] \u001b[39m=\u001b[39m all_results\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "all_result_types = {}\n",
    "for text_type,sub_dataset in datasets.items():\n",
    "    results_sub = {}\n",
    "    for model_name, dataset in sub_dataset.items():\n",
    "        all_results = []\n",
    "        for out in tqdm(pipe_translate(dataset, batch_size=batch_size), total=len(dataset)):\n",
    "            #assert len(out) == 1\n",
    "            all_results.append(out[0]['translation_text'])\n",
    "        results_sub[model_name] = all_results\n",
    "    all_result_types[text_type] = results_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
