{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/content_understanding/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-08 22:29:09.331202: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnthropicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, item_to_ret='prompt'):\n",
    "        self.hf_dataset = load_dataset('Anthropic/hh-rlhf', split=split)\n",
    "        self.prompts, self.chosen, self.reject = [], [], []\n",
    "        for d in (self.hf_dataset):\n",
    "            try:\n",
    "                prompt, chosen, reject = self.process_response(d)\n",
    "                self.prompts.append(prompt)\n",
    "                self.chosen.append(chosen)\n",
    "                self.reject.append(reject)\n",
    "            except:\n",
    "                continue\n",
    "        self.set_dataset_type(item_to_ret)\n",
    "\n",
    "    def set_dataset_type(self, item_to_ret):\n",
    "        if item_to_ret == 'prompt':\n",
    "            self.dataset = self.prompts\n",
    "        elif item_to_ret == 'chosen':\n",
    "            self.dataset = self.chosen\n",
    "        elif item_to_ret == 'rejected':\n",
    "            self.dataset = self.reject\n",
    "        else:\n",
    "            self.dataset = list(zip(self.prompts, self.chosen, self.reject))\n",
    "\n",
    "    def process_response(self, x):\n",
    "        chosen = x['chosen']\n",
    "        reject = x['rejected']\n",
    "        ind = chosen.rfind('\\n\\nAssistant:')\n",
    "        prompt = chosen[:ind].strip()\n",
    "        assert reject[:len(prompt)] == prompt\n",
    "        chosen = chosen[ind + len('\\n\\nAssistant:'):].strip()\n",
    "        reject = reject[ind + len('\\n\\nAssistant:'):].strip()\n",
    "        return prompt, chosen, reject\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, item_to_ret='prompt'):\n",
    "        self.hf_dataset = load_dataset('openai/summarize_from_feedback', 'comparisons', split=split)\n",
    "        self.prompts, self.chosen, self.reject = [], [], []\n",
    "        for d in (self.hf_dataset):\n",
    "            try:\n",
    "                prompt, chosen, reject = self.process_response(d)\n",
    "                self.prompts.append(prompt)\n",
    "                self.chosen.append(chosen)\n",
    "                self.reject.append(reject)\n",
    "            except:\n",
    "                continue\n",
    "        self.set_dataset_type(item_to_ret)\n",
    "\n",
    "    def set_dataset_type(self, item_to_ret):\n",
    "        if item_to_ret == 'prompt':\n",
    "            self.dataset = self.prompts\n",
    "        elif item_to_ret == 'chosen':\n",
    "            self.dataset = self.chosen\n",
    "        elif item_to_ret == 'rejected':\n",
    "            self.dataset = self.reject\n",
    "        else:\n",
    "            self.dataset = list(zip(self.prompts, self.chosen, self.reject))\n",
    "\n",
    "\n",
    "    def process_response(self, x):\n",
    "        prompt = x['info']['post'].strip()\n",
    "        first = x['summaries'][0]['text'].strip()\n",
    "        second = x['summaries'][1]['text'].strip()\n",
    "        choice = x['choice']\n",
    "        if choice == 0:\n",
    "            return prompt, first, second\n",
    "        elif choice == 1:\n",
    "            return prompt, second, first\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.og_dataset = dataset\n",
    "        self.sentences = []\n",
    "        self.sentence_to_prompt = []\n",
    "        for i,x in enumerate(self.og_dataset):\n",
    "            sents = self.break_sentences(x)\n",
    "            self.sentences.extend(sents)\n",
    "            self.sentence_to_prompt.extend([i] * len(sents))\n",
    "        assert len(self.sentence_to_prompt) == len(self.sentences)\n",
    "\n",
    "    def break_sentences(self, x):\n",
    "        return sent_tokenize(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deberta_test_datasets():\n",
    "    dataset = {}\n",
    "    for t in ['prompt', 'chosen', 'rejected']:\n",
    "        sub_dataset = {}\n",
    "        sub_dataset['openai/summarize_from_feedback'] = SummaryDataset('validation', item_to_ret=t)\n",
    "        # sub_dataset = load_dataset('openai/webgpt_comparisons')\n",
    "        # sub_dataset load_dataset('Dahoas/synthetic-instruct-gptj-pairwise')\n",
    "        #sub_dataset['Anthropic/hh-rlhf'] = load_dataset('Anthropic/hh-rlhf', split='test', item_to_ret=t)\n",
    "        for k,v in sub_dataset.items():\n",
    "            sub_dataset[k] = SentenceDataset(v)\n",
    "        dataset[t] = sub_dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset summarize_from_feedback (/home/ubuntu/.cache/huggingface/datasets/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n",
      "Found cached dataset summarize_from_feedback (/home/ubuntu/.cache/huggingface/datasets/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n",
      "Found cached dataset summarize_from_feedback (/home/ubuntu/.cache/huggingface/datasets/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n"
     ]
    }
   ],
   "source": [
    "datasets = get_deberta_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "prompt\n",
      "openai/summarize_from_feedback 1141862\n",
      "chosen\n",
      "openai/summarize_from_feedback 172862\n",
      "rejected\n",
      "openai/summarize_from_feedback 162461\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets))\n",
    "for k,vs in datasets.items():\n",
    "    print(k)\n",
    "    for k_,v in vs.items():\n",
    "        print(k_, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'device_map')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#tokenizer_translate = AutoTokenizer.from_pretrained(model_name)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#model_translate = AutoModelForSeq2SeqLM.from_pretrained(model_name)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pipe_translate \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtranslation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49mmodel_name, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/transformers/pipelines/__init__.py:788\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    787\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 788\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    789\u001b[0m         model,\n\u001b[1;32m    790\u001b[0m         model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    791\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    792\u001b[0m         framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    793\u001b[0m         task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    794\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    795\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    796\u001b[0m     )\n\u001b[1;32m    798\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    799\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/transformers/pipelines/base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    271\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    492\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    494\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    495\u001b[0m     )\n\u001b[1;32m    496\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:2874\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mload_weight_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m load_weight_prefix \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m model_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2873\u001b[0m \u001b[39m# Instantiate model.\u001b[39;00m\n\u001b[0;32m-> 2874\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2876\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[1;32m   2877\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_pytorch_checkpoint_in_tf2_model\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/transformers/models/marian/modeling_tf_marian.py:1269\u001b[0m, in \u001b[0;36mTFMarianMTModel.__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1269\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m TFMarianMainLayer(config, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39muse_cache\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:1141\u001b[0m, in \u001b[0;36mTFPreTrainedModel.__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1141\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   1143\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter config in `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m(config)` should be an instance of class \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`PretrainedConfig`. To create a model from a pretrained model use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1146\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`model = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1147\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/keras/utils/generic_utils.py:515\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 515\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'device_map')"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "#tokenizer_translate = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_translate = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "pipe_translate = pipeline(\"translation\", model=model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1141862 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m all_results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m tqdm(pipe_translate(dataset, batch_size\u001b[39m=\u001b[39mbatch_size), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     all_results\u001b[39m.\u001b[39mappend(out[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtranslation_text\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/neurips2023_distshift/translate_datasets.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m results_sub[model_name] \u001b[39m=\u001b[39m all_results\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "all_result_types = {}\n",
    "for text_type,sub_dataset in datasets.items():\n",
    "    results_sub = {}\n",
    "    for model_name, dataset in sub_dataset.items():\n",
    "        all_results = []\n",
    "        for out in tqdm(pipe_translate(dataset, batch_size=batch_size), total=len(dataset)):\n",
    "            #assert len(out) == 1\n",
    "            all_results.append(out[0]['translation_text'])\n",
    "        results_sub[model_name] = all_results\n",
    "    all_result_types[text_type] = results_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content_understanding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
